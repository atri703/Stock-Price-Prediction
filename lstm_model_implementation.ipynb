{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfusePo6KUIW"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_m9mWFOJeG0"
      },
      "outputs": [],
      "source": [
        "# To clear GPU memory\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PV-KuJsKWCc"
      },
      "outputs": [],
      "source": [
        "from pandas_datareader import DataReader\n",
        "from pandas_datareader import data as pdr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIUoC_VtKXjo"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv7jLAQwn9Iw"
      },
      "source": [
        "## Case 1: Large Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuAU_h6-orqA"
      },
      "source": [
        "### Importing data from yahoo finance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lz7lbW3K-aV"
      },
      "outputs": [],
      "source": [
        "start = \"2007-01-01\"\n",
        "end = \"2023-07-31\"\n",
        "\n",
        "# Convert start and end dates to datetime objects\n",
        "start_date = datetime.strptime(start, \"%Y-%m-%d\")\n",
        "end_date = datetime.strptime(end, \"%Y-%m-%d\")\n",
        "\n",
        "# Set up the data reader with Yahoo Finance\n",
        "yf.pdr_override()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IRRAlPLKY5W"
      },
      "outputs": [],
      "source": [
        "# Making an array for different stock prices from tech industry\n",
        "# AAPL - Apple\n",
        "# MSFT - Microsoft\n",
        "# AMZN - Amazon\n",
        "# NFLX - Netflix\n",
        "# INFY - Infosys\n",
        "# ADBE - Adobe\n",
        "# GOOGL - Google (Class A - gives voting rights)\n",
        "# NVDA - NVIDIA Corporation\n",
        "\n",
        "stock_types = [\"AAPL\", \"MSFT\", \"AMZN\", \"NFLX\", \"INFY\", \"ADBE\", \"GOOGL\", \"NVDA\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u02ez7e_Khqg"
      },
      "outputs": [],
      "source": [
        "# Making dictionary for every stock\n",
        "stock_dict = {}\n",
        "for stock in stock_types:\n",
        "  stock_dict[stock] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puoh_i6HKm07",
        "outputId": "236837a3-cf3b-4ffc-e5b9-24b321b52f40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AAPL\n",
            "MSFT\n",
            "AMZN\n",
            "NFLX\n",
            "INFY\n",
            "ADBE\n",
            "GOOGL\n",
            "NVDA\n"
          ]
        }
      ],
      "source": [
        "for stock in stock_dict:\n",
        "  print(stock)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUw6_EwJKpjO",
        "outputId": "7620f120-b285-4689-c8a2-4c94539aeec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r",
            "[*********************100%***********************]  1 of 1 completed\n",
            "AAPL-\n",
            "        Date      Open      High       Low     Close  Adj Close      Volume\n",
            "0 2007-01-03  3.081786  3.092143  2.925000  2.992857   2.540327  1238319600\n",
            "1 2007-01-04  3.001786  3.069643  2.993571  3.059286   2.596711   847260400\n",
            "2 2007-01-05  3.063214  3.078571  3.014286  3.037500   2.578220   834741600\n",
            "3 2007-01-08  3.070000  3.090357  3.045714  3.052500   2.590951   797106800\n",
            "4 2007-01-09  3.087500  3.320714  3.041071  3.306071   2.806182  3349298400\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "MSFT-\n",
            "        Date       Open       High        Low      Close  Adj Close    Volume\n",
            "0 2007-01-03  29.910000  30.250000  29.400000  29.860001  21.525984  76935100\n",
            "1 2007-01-04  29.700001  29.969999  29.440001  29.809999  21.489935  45774500\n",
            "2 2007-01-05  29.629999  29.750000  29.450001  29.639999  21.367382  44607200\n",
            "3 2007-01-08  29.650000  30.100000  29.530001  29.930000  21.576443  50220200\n",
            "4 2007-01-09  30.000000  30.180000  29.730000  29.959999  21.598076  44636600\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "AMZN-\n",
            "        Date    Open    High     Low   Close  Adj Close     Volume\n",
            "0 2007-01-03  1.9340  1.9530  1.9025  1.9350     1.9350  248102000\n",
            "1 2007-01-04  1.9295  1.9570  1.9130  1.9450     1.9450  126368000\n",
            "2 2007-01-05  1.9360  1.9395  1.8800  1.9185     1.9185  132394000\n",
            "3 2007-01-08  1.9110  1.9155  1.8585  1.8750     1.8750  135660000\n",
            "4 2007-01-09  1.8800  1.9030  1.8670  1.8890     1.8890  114060000\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "NFLX-\n",
            "        Date      Open      High       Low     Close  Adj Close    Volume\n",
            "0 2007-01-03  3.714286  3.824286  3.677143  3.801429   3.801429  16440900\n",
            "1 2007-01-04  3.772857  3.828571  3.585714  3.621429   3.621429  15959300\n",
            "2 2007-01-05  3.620000  3.620000  3.492857  3.544286   3.544286  15190700\n",
            "3 2007-01-08  3.545714  3.555714  3.367143  3.404286   3.404286  18344900\n",
            "4 2007-01-09  3.427143  3.440000  3.360000  3.427143   3.427143  10611300\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "INFY-\n",
            "        Date     Open     High      Low   Close  Adj Close    Volume\n",
            "0 2007-01-03  6.94125  7.03500  6.84250  6.9775   4.907321  24904800\n",
            "1 2007-01-04  6.95125  6.97375  6.85625  6.9250   4.870399  14812800\n",
            "2 2007-01-05  6.88125  6.93750  6.83000  6.8925   4.847541  11834400\n",
            "3 2007-01-08  6.82500  6.92875  6.80000  6.8750   4.835232  12805600\n",
            "4 2007-01-09  6.87500  6.87500  6.75000  6.8250   4.800069  16294400\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "ADBE-\n",
            "        Date       Open       High        Low      Close  Adj Close   Volume\n",
            "0 2007-01-03  40.720001  41.320000  38.889999  39.919998  39.919998  7126000\n",
            "1 2007-01-04  39.880001  41.000000  39.430000  40.820000  40.820000  4503700\n",
            "2 2007-01-05  40.779999  40.900002  40.119999  40.619999  40.619999  2730200\n",
            "3 2007-01-08  40.410000  40.970001  40.130001  40.450001  40.450001  5234800\n",
            "4 2007-01-09  40.500000  40.509998  39.380001  39.630001  39.630001  5672900\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "GOOGL-\n",
            "        Date       Open       High        Low      Close  Adj Close     Volume\n",
            "0 2007-01-03  11.661662  11.928428  11.539289  11.701451  11.701451  307951740\n",
            "1 2007-01-04  11.736737  12.110861  11.720470  12.093594  12.093594  315188496\n",
            "2 2007-01-05  12.074575  12.199700  11.964715  12.191942  12.191942  274609116\n",
            "3 2007-01-08  12.204454  12.259009  12.067067  12.101602  12.101602  189985824\n",
            "4 2007-01-09  12.148398  12.218468  12.042042  12.149650  12.149650  215040744\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "NVDA-\n",
            "        Date      Open      High       Low     Close  Adj Close     Volume\n",
            "0 2007-01-03  6.178333  6.253333  5.798333  6.013333   5.517242  115482000\n",
            "1 2007-01-04  5.991667  6.013333  5.838333  5.985000   5.491247   79729800\n",
            "2 2007-01-05  5.843333  5.866667  5.570000  5.610000   5.147183  124334400\n",
            "3 2007-01-08  5.630000  5.760000  5.533333  5.651667   5.185414   65727000\n",
            "4 2007-01-09  5.660000  5.698333  5.535000  5.541667   5.084488   76416600\n"
          ]
        }
      ],
      "source": [
        "for stock in stock_dict:\n",
        "  # Fetch the data using DataReader\n",
        "  df = pdr.get_data_yahoo(stock, start=start_date, end=end_date)\n",
        "  df = df.reset_index()\n",
        "\n",
        "  if df is not None:\n",
        "    stock_dict[stock] = df\n",
        "    print(stock + \"-\" )\n",
        "    print(df.head())  # Display the first few rows of the loaded data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jr648T2K4-B",
        "outputId": "8cbea8db-b959-4470-a415-c7d697a7d1f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4171"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stock_dict[\"AAPL\"].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK4kIk5rLFAt",
        "outputId": "56a0dbda-bc41-480b-bb57-17b7dcbc145c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AAPL-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4171 entries, 0 to 4170\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       4171 non-null   datetime64[ns]\n",
            " 1   Open       4171 non-null   float64       \n",
            " 2   High       4171 non-null   float64       \n",
            " 3   Low        4171 non-null   float64       \n",
            " 4   Close      4171 non-null   float64       \n",
            " 5   Adj Close  4171 non-null   float64       \n",
            " 6   Volume     4171 non-null   int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 228.2 KB\n",
            "None\n",
            "              Open         High          Low        Close    Adj Close  \\\n",
            "count  4171.000000  4171.000000  4171.000000  4171.000000  4171.000000   \n",
            "mean     46.785610    47.307183    46.286132    46.818624    45.051682   \n",
            "std      50.777082    51.386761    50.223518    50.833700    50.967575   \n",
            "min       2.835357     2.928571     2.792857     2.792857     2.370567   \n",
            "25%      11.973572    12.064464    11.856429    11.969286    10.159489   \n",
            "50%      26.102501    26.412500    25.920000    26.145000    23.810760   \n",
            "75%      52.597500    53.260000    52.096251    52.573751    50.818949   \n",
            "max     196.020004   198.229996   194.139999   195.830002   195.565918   \n",
            "\n",
            "             Volume  \n",
            "count  4.171000e+03  \n",
            "mean   3.652647e+08  \n",
            "std    3.719504e+08  \n",
            "min    3.145820e+07  \n",
            "25%    1.062793e+08  \n",
            "50%    2.125872e+08  \n",
            "75%    4.952276e+08  \n",
            "max    3.372970e+09  \n",
            "MSFT-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4171 entries, 0 to 4170\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       4171 non-null   datetime64[ns]\n",
            " 1   Open       4171 non-null   float64       \n",
            " 2   High       4171 non-null   float64       \n",
            " 3   Low        4171 non-null   float64       \n",
            " 4   Close      4171 non-null   float64       \n",
            " 5   Adj Close  4171 non-null   float64       \n",
            " 6   Volume     4171 non-null   int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 228.2 KB\n",
            "None\n",
            "              Open         High          Low        Close    Adj Close  \\\n",
            "count  4171.000000  4171.000000  4171.000000  4171.000000  4171.000000   \n",
            "mean     93.348092    94.301544    92.374064    93.381848    87.991299   \n",
            "std      91.373879    92.356014    90.364707    91.409984    92.450222   \n",
            "min      15.200000    15.620000    14.870000    15.150000    11.351553   \n",
            "25%      28.884999    29.175000    28.610001    28.920000    22.043162   \n",
            "50%      46.259998    46.720001    45.799999    46.290001    40.314270   \n",
            "75%     130.294998   131.669998   129.370003   131.000000   125.679981   \n",
            "max     361.750000   366.779999   352.440002   359.489990   359.489990   \n",
            "\n",
            "             Volume  \n",
            "count  4.171000e+03  \n",
            "mean   4.349909e+07  \n",
            "std    2.661554e+07  \n",
            "min    7.425600e+06  \n",
            "25%    2.543185e+07  \n",
            "50%    3.602760e+07  \n",
            "75%    5.378325e+07  \n",
            "max    3.193179e+08  \n",
            "AMZN-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4171 entries, 0 to 4170\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       4171 non-null   datetime64[ns]\n",
            " 1   Open       4171 non-null   float64       \n",
            " 2   High       4171 non-null   float64       \n",
            " 3   Low        4171 non-null   float64       \n",
            " 4   Close      4171 non-null   float64       \n",
            " 5   Adj Close  4171 non-null   float64       \n",
            " 6   Volume     4171 non-null   int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 228.2 KB\n",
            "None\n",
            "              Open         High          Low        Close    Adj Close  \\\n",
            "count  4171.000000  4171.000000  4171.000000  4171.000000  4171.000000   \n",
            "mean     50.638288    51.235369    49.986557    50.621819    50.621819   \n",
            "std      53.606474    54.248216    52.897852    53.566065    53.566065   \n",
            "min       1.764500     1.853500     1.734000     1.751500     1.751500   \n",
            "25%       8.910500     8.992500     8.801750     8.902250     8.902250   \n",
            "50%      20.126499    20.281500    19.840500    20.110001    20.110001   \n",
            "75%      90.050003    90.904499    89.029999    90.030750    90.030750   \n",
            "max     187.199997   188.654007   184.839493   186.570496   186.570496   \n",
            "\n",
            "             Volume  \n",
            "count  4.171000e+03  \n",
            "mean   1.055939e+08  \n",
            "std    8.590339e+07  \n",
            "min    1.762600e+07  \n",
            "25%    5.910500e+07  \n",
            "50%    8.356600e+07  \n",
            "75%    1.263610e+08  \n",
            "max    2.086584e+09  \n",
            "NFLX-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4171 entries, 0 to 4170\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       4171 non-null   datetime64[ns]\n",
            " 1   Open       4171 non-null   float64       \n",
            " 2   High       4171 non-null   float64       \n",
            " 3   Low        4171 non-null   float64       \n",
            " 4   Close      4171 non-null   float64       \n",
            " 5   Adj Close  4171 non-null   float64       \n",
            " 6   Volume     4171 non-null   int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 228.2 KB\n",
            "None\n",
            "              Open         High          Low        Close    Adj Close  \\\n",
            "count  4171.000000  4171.000000  4171.000000  4171.000000  4171.000000   \n",
            "mean    160.886902   163.406330   158.279457   160.921202   160.921202   \n",
            "std     177.743351   180.300346   175.007551   177.699877   177.699877   \n",
            "min       2.368571     2.394286     2.231429     2.295714     2.295714   \n",
            "25%      12.831429    13.248571    12.674285    12.939285    12.939285   \n",
            "50%      76.000000    79.714287    75.714287    79.271431    79.271431   \n",
            "75%     307.235001   312.699997   302.205002   307.705002   307.705002   \n",
            "max     692.349976   700.989990   686.090027   691.690002   691.690002   \n",
            "\n",
            "             Volume  \n",
            "count  4.171000e+03  \n",
            "mean   1.674753e+07  \n",
            "std    1.882630e+07  \n",
            "min    1.144000e+06  \n",
            "25%    6.292350e+06  \n",
            "50%    1.069530e+07  \n",
            "75%    2.015995e+07  \n",
            "max    3.155418e+08  \n",
            "INFY-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4171 entries, 0 to 4170\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       4171 non-null   datetime64[ns]\n",
            " 1   Open       4171 non-null   float64       \n",
            " 2   High       4171 non-null   float64       \n",
            " 3   Low        4171 non-null   float64       \n",
            " 4   Close      4171 non-null   float64       \n",
            " 5   Adj Close  4171 non-null   float64       \n",
            " 6   Volume     4171 non-null   int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 228.2 KB\n",
            "None\n",
            "              Open         High          Low        Close    Adj Close  \\\n",
            "count  4171.000000  4171.000000  4171.000000  4171.000000  4171.000000   \n",
            "mean      9.484199     9.571951     9.397160     9.487276     8.201134   \n",
            "std       4.935401     4.969488     4.902912     4.937906     5.217276   \n",
            "min       2.751250     2.815000     2.637500     2.638750     1.909761   \n",
            "25%       6.493125     6.560625     6.436250     6.502500     4.819452   \n",
            "50%       7.865000     7.930000     7.807500     7.867500     6.363008   \n",
            "75%      10.195000    10.282500    10.120000    10.197500     9.169782   \n",
            "max      26.150000    26.389999    25.580000    26.200001    25.289547   \n",
            "\n",
            "             Volume  \n",
            "count  4.171000e+03  \n",
            "mean   1.328418e+07  \n",
            "std    9.039359e+06  \n",
            "min    1.068400e+06  \n",
            "25%    7.679600e+06  \n",
            "50%    1.086900e+07  \n",
            "75%    1.606500e+07  \n",
            "max    1.475912e+08  \n",
            "ADBE-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4171 entries, 0 to 4170\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       4171 non-null   datetime64[ns]\n",
            " 1   Open       4171 non-null   float64       \n",
            " 2   High       4171 non-null   float64       \n",
            " 3   Low        4171 non-null   float64       \n",
            " 4   Close      4171 non-null   float64       \n",
            " 5   Adj Close  4171 non-null   float64       \n",
            " 6   Volume     4171 non-null   int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 228.2 KB\n",
            "None\n",
            "              Open         High          Low        Close    Adj Close  \\\n",
            "count  4171.000000  4171.000000  4171.000000  4171.000000  4171.000000   \n",
            "mean    165.926852   167.935636   163.786142   165.959386   165.959386   \n",
            "std     170.260714   172.353743   167.945796   170.233992   170.233992   \n",
            "min      15.710000    16.480000    15.700000    15.980000    15.980000   \n",
            "25%      35.705000    36.049999    35.270000    35.680000    35.680000   \n",
            "50%      76.430000    77.360001    75.839996    76.550003    76.550003   \n",
            "75%     275.145004   277.800003   272.309998   275.544998   275.544998   \n",
            "max     696.280029   699.539978   678.909973   688.369995   688.369995   \n",
            "\n",
            "             Volume  \n",
            "count  4.171000e+03  \n",
            "mean   4.442981e+06  \n",
            "std    3.769514e+06  \n",
            "min    5.892000e+05  \n",
            "25%    2.314700e+06  \n",
            "50%    3.419000e+06  \n",
            "75%    5.487450e+06  \n",
            "max    1.087524e+08  \n",
            "GOOGL-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4171 entries, 0 to 4170\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       4171 non-null   datetime64[ns]\n",
            " 1   Open       4171 non-null   float64       \n",
            " 2   High       4171 non-null   float64       \n",
            " 3   Low        4171 non-null   float64       \n",
            " 4   Close      4171 non-null   float64       \n",
            " 5   Adj Close  4171 non-null   float64       \n",
            " 6   Volume     4171 non-null   int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 228.2 KB\n",
            "None\n",
            "              Open         High          Low        Close    Adj Close  \\\n",
            "count  4171.000000  4171.000000  4171.000000  4171.000000  4171.000000   \n",
            "mean     44.432344    44.897935    43.965736    44.441931    44.441931   \n",
            "std      36.831065    37.258989    36.417627    36.845015    36.845015   \n",
            "min       6.569319     6.740991     6.188689     6.442442     6.442442   \n",
            "25%      14.706707    14.819819    14.552928    14.673048    14.673048   \n",
            "50%      29.509510    29.686001    29.302000    29.476000    29.476000   \n",
            "75%      59.730249    60.229000    59.133251    59.698750    59.698750   \n",
            "max     151.250000   151.546494   148.899002   149.838501   149.838501   \n",
            "\n",
            "             Volume  \n",
            "count  4.171000e+03  \n",
            "mean   8.533727e+07  \n",
            "std    8.488818e+07  \n",
            "min    9.312000e+06  \n",
            "25%    3.088700e+07  \n",
            "50%    5.105000e+07  \n",
            "75%    1.076922e+08  \n",
            "max    9.305605e+08  \n",
            "NVDA-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4171 entries, 0 to 4170\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       4171 non-null   datetime64[ns]\n",
            " 1   Open       4171 non-null   float64       \n",
            " 2   High       4171 non-null   float64       \n",
            " 3   Low        4171 non-null   float64       \n",
            " 4   Close      4171 non-null   float64       \n",
            " 5   Adj Close  4171 non-null   float64       \n",
            " 6   Volume     4171 non-null   int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 228.2 KB\n",
            "None\n",
            "              Open         High          Low        Close    Adj Close  \\\n",
            "count  4171.000000  4171.000000  4171.000000  4171.000000  4171.000000   \n",
            "mean     50.673241    51.626661    49.701115    50.706136    50.411435   \n",
            "std      80.902343    82.487780    79.288116    80.944527    80.978281   \n",
            "min       1.500000     1.595000     1.437500     1.475000     1.353315   \n",
            "25%       3.805000     3.856250     3.752500     3.805000     3.532250   \n",
            "50%       6.922500     7.050000     6.752500     6.885000     6.560136   \n",
            "75%      59.123751    60.028751    57.947500    59.090000    58.625938   \n",
            "max     474.640015   480.880005   467.420013   474.940002   474.940002   \n",
            "\n",
            "             Volume  \n",
            "count  4.171000e+03  \n",
            "mean   5.470340e+07  \n",
            "std    3.277858e+07  \n",
            "min    4.564400e+06  \n",
            "25%    3.295780e+07  \n",
            "50%    4.755880e+07  \n",
            "75%    6.752400e+07  \n",
            "max    3.692928e+08  \n"
          ]
        }
      ],
      "source": [
        "for stock in stock_dict:\n",
        "  print(stock + \"-\")\n",
        "  print(stock_dict[stock].info())\n",
        "  print(stock_dict[stock].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLpN2lzOLfc3"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br4QTu2zLxS1"
      },
      "outputs": [],
      "source": [
        "dataset = {}\n",
        "\n",
        "for stock in stock_dict:\n",
        "\n",
        "  #Creating a new dataframe with only the 'Close' column\n",
        "  data = stock_dict[stock].filter(['Close'])\n",
        "\n",
        "  #Converting the dataframe to a numpy array\n",
        "  dataset[stock] = data.values\n",
        "  dataset[stock] = dataset[stock].reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snlCUiRnMkjW",
        "outputId": "b1092c0b-d7e7-402e-8b15-ce4c510ee6f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3337"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Get /Compute the number of rows to train the model on\n",
        "training_data_len = math.ceil( len(dataset[stock]) *.8)\n",
        "training_data_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl2DhxTY8cla"
      },
      "source": [
        "### Scalling Data for better results and making it suiatble for LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB_3fMwlMD-S"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d8Vyc_kNCRD"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler(feature_range=(0, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIkhTum6MGWj"
      },
      "outputs": [],
      "source": [
        "scaled_data_dict = {}\n",
        "\n",
        "for stock in dataset:\n",
        "  # here we are Scaling the all of the data to be values between 0 and 1\n",
        "  scaled_data_dict[stock] = scaler.fit_transform(dataset[stock])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH_VvRBDNDyy",
        "outputId": "61cf1363-2cbe-4118-9b2b-b5130751cfe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.00103607]\n",
            " [0.0013802 ]\n",
            " [0.00126734]\n",
            " ...\n",
            " [0.99311012]\n",
            " [0.98647928]\n",
            " [1.        ]]\n"
          ]
        }
      ],
      "source": [
        "print(scaled_data_dict[\"AAPL\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s17u0ipMNL2N"
      },
      "outputs": [],
      "source": [
        "train_data_dict = {}\n",
        "x_train_dict = {}\n",
        "y_train_dict = {}\n",
        "\n",
        "for stock in scaled_data_dict:\n",
        "  #Creating the scaled training data set\n",
        "  train_data_dict[stock] = scaled_data_dict[stock][0:training_data_len  , : ]\n",
        "  #Spliting the data into x_train and y_train data sets\n",
        "  x_train=[]\n",
        "  y_train = []\n",
        "  for i in range(60,len(train_data_dict[stock])):\n",
        "    x_train.append(train_data_dict[stock][i-60:i,0])\n",
        "    y_train.append(train_data_dict[stock][i,0])\n",
        "\n",
        "  #Here we are Converting x_train and y_train to numpy arrays\n",
        "  x_train_dict[stock] = np.array(x_train)\n",
        "  y_train_dict[stock] = np.array(y_train)\n",
        "\n",
        "  # Here we are reshaping the data into the shape accepted by the LSTM\n",
        "  x_train_dict[stock] = np.reshape(x_train_dict[stock], (x_train_dict[stock].shape[0], x_train_dict[stock].shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OaXiZ62N2J6",
        "outputId": "fc528691-7e26-4def-da5a-cd95d7f36a01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3277, 60, 1)"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train_dict[\"AAPL\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcMcGCfbOtG_"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LSTM, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNsPaYEKPeUu",
        "outputId": "a7de4559-a72b-44c5-fbe9-b5e6a909c59c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AAPL -\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 60, 50)            10400     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 50)                20200     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 25)                1275      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 26        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,901\n",
            "Trainable params: 31,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "3277/3277 [==============================] - 36s 10ms/step - loss: 1.9533e-04\n",
            "Epoch 2/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 7.4684e-05\n",
            "Epoch 3/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 5.3806e-05\n",
            "Epoch 4/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 5.8741e-05\n",
            "Epoch 5/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 4.3882e-05\n",
            "Epoch 6/20\n",
            "3277/3277 [==============================] - 33s 10ms/step - loss: 3.9513e-05\n",
            "Epoch 7/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 3.2362e-05\n",
            "Epoch 8/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 3.8343e-05\n",
            "Epoch 9/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 3.7010e-05\n",
            "Epoch 10/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 3.3871e-05\n",
            "Epoch 11/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 2.8961e-05\n",
            "Epoch 12/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 2.8268e-05\n",
            "Epoch 13/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 2.7071e-05\n",
            "Epoch 14/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 2.9808e-05\n",
            "Epoch 15/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 2.6164e-05\n",
            "Epoch 16/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 2.6131e-05\n",
            "Epoch 17/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 2.3599e-05\n",
            "Epoch 18/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 2.6887e-05\n",
            "Epoch 19/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 2.6608e-05\n",
            "Epoch 20/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 2.4110e-05\n",
            "\n",
            "MSFT -\n",
            "Epoch 1/20\n",
            "3277/3277 [==============================] - 38s 10ms/step - loss: 2.2743e-04\n",
            "Epoch 2/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 1.5319e-04\n",
            "Epoch 3/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 9.0849e-05\n",
            "Epoch 4/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 5.9112e-05\n",
            "Epoch 5/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 6.5207e-05\n",
            "Epoch 6/20\n",
            "3277/3277 [==============================] - 33s 10ms/step - loss: 5.5642e-05\n",
            "Epoch 7/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 5.1332e-05\n",
            "Epoch 8/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 4.5111e-05\n",
            "Epoch 9/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 4.6154e-05\n",
            "Epoch 10/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 3.8665e-05\n",
            "Epoch 11/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 3.8535e-05\n",
            "Epoch 12/20\n",
            "3277/3277 [==============================] - 33s 10ms/step - loss: 3.7281e-05\n",
            "Epoch 13/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 3.7806e-05\n",
            "Epoch 14/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 3.7938e-05\n",
            "Epoch 15/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 3.3908e-05\n",
            "Epoch 16/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 3.5380e-05\n",
            "Epoch 17/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 3.1467e-05\n",
            "Epoch 18/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 3.9055e-05\n",
            "Epoch 19/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 3.2096e-05\n",
            "Epoch 20/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 3.4109e-05\n",
            "\n",
            "AMZN -\n",
            "Epoch 1/20\n",
            "3277/3277 [==============================] - 36s 10ms/step - loss: 3.8408e-04\n",
            "Epoch 2/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 1.5995e-04\n",
            "Epoch 3/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 1.5850e-04\n",
            "Epoch 4/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 9.1737e-05\n",
            "Epoch 5/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 8.7960e-05\n",
            "Epoch 6/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 8.4957e-05\n",
            "Epoch 7/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 8.1488e-05\n",
            "Epoch 8/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 7.0804e-05\n",
            "Epoch 9/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 6.1632e-05\n",
            "Epoch 10/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 5.9798e-05\n",
            "Epoch 11/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 6.2514e-05\n",
            "Epoch 12/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 5.8071e-05\n",
            "Epoch 13/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 6.2443e-05\n",
            "Epoch 14/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 5.4353e-05\n",
            "Epoch 15/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 5.2420e-05\n",
            "Epoch 16/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 5.9927e-05\n",
            "Epoch 17/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 4.9840e-05\n",
            "Epoch 18/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 5.5345e-05\n",
            "Epoch 19/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 4.9950e-05\n",
            "Epoch 20/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 5.0025e-05\n",
            "\n",
            "NFLX -\n",
            "Epoch 1/20\n",
            "3277/3277 [==============================] - 34s 9ms/step - loss: 6.6012e-04\n",
            "Epoch 2/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 2.3966e-04\n",
            "Epoch 3/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 2.0484e-04\n",
            "Epoch 4/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 1.3030e-04\n",
            "Epoch 5/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 1.3157e-04\n",
            "Epoch 6/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 1.0933e-04\n",
            "Epoch 7/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 1.0317e-04\n",
            "Epoch 8/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 1.0239e-04\n",
            "Epoch 9/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 9.8911e-05\n",
            "Epoch 10/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 9.4517e-05\n",
            "Epoch 11/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 9.9537e-05\n",
            "Epoch 12/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 8.6909e-05\n",
            "Epoch 13/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 8.4710e-05\n",
            "Epoch 14/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 8.4921e-05\n",
            "Epoch 15/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 8.2251e-05\n",
            "Epoch 16/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 7.2947e-05\n",
            "Epoch 17/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 8.5312e-05\n",
            "Epoch 18/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 7.1935e-05\n",
            "Epoch 19/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 7.9494e-05\n",
            "Epoch 20/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 7.5407e-05\n",
            "\n",
            "INFY -\n",
            "Epoch 1/20\n",
            "3277/3277 [==============================] - 34s 9ms/step - loss: 3.0954e-04\n",
            "Epoch 2/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 1.1391e-04\n",
            "Epoch 3/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 9.1486e-05\n",
            "Epoch 4/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 8.2463e-05\n",
            "Epoch 5/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 8.0033e-05\n",
            "Epoch 6/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 6.8967e-05\n",
            "Epoch 7/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 7.3818e-05\n",
            "Epoch 8/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 6.9306e-05\n",
            "Epoch 9/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 6.8347e-05\n",
            "Epoch 10/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 6.5648e-05\n",
            "Epoch 11/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 6.2399e-05\n",
            "Epoch 12/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 6.7986e-05\n",
            "Epoch 13/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 6.2617e-05\n",
            "Epoch 14/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 6.1417e-05\n",
            "Epoch 15/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 5.9061e-05\n",
            "Epoch 16/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 6.0738e-05\n",
            "Epoch 17/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 5.9873e-05\n",
            "Epoch 18/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 6.0005e-05\n",
            "Epoch 19/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 5.7198e-05\n",
            "Epoch 20/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 5.7811e-05\n",
            "\n",
            "ADBE -\n",
            "Epoch 1/20\n",
            "3277/3277 [==============================] - 36s 10ms/step - loss: 3.1639e-04\n",
            "Epoch 2/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 1.4379e-04\n",
            "Epoch 3/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 1.0770e-04\n",
            "Epoch 4/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 1.1421e-04\n",
            "Epoch 5/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 6.6558e-05\n",
            "Epoch 6/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 6.7020e-05\n",
            "Epoch 7/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 5.6064e-05\n",
            "Epoch 8/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 6.0252e-05\n",
            "Epoch 9/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 5.1812e-05\n",
            "Epoch 10/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 5.4092e-05\n",
            "Epoch 11/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 5.4248e-05\n",
            "Epoch 12/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 5.9750e-05\n",
            "Epoch 13/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 4.4718e-05\n",
            "Epoch 14/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 5.1220e-05\n",
            "Epoch 15/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 4.7915e-05\n",
            "Epoch 16/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 4.2580e-05\n",
            "Epoch 17/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 4.3045e-05\n",
            "Epoch 18/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 4.6438e-05\n",
            "Epoch 19/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 4.2118e-05\n",
            "Epoch 20/20\n",
            "3277/3277 [==============================] - 30s 9ms/step - loss: 4.4536e-05\n",
            "\n",
            "GOOGL -\n",
            "Epoch 1/20\n",
            "3277/3277 [==============================] - 34s 9ms/step - loss: 2.8083e-04\n",
            "Epoch 2/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 1.1241e-04\n",
            "Epoch 3/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 7.4872e-05\n",
            "Epoch 4/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 7.9566e-05\n",
            "Epoch 5/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 5.9757e-05\n",
            "Epoch 6/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 5.5766e-05\n",
            "Epoch 7/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 5.1339e-05\n",
            "Epoch 8/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 4.4770e-05\n",
            "Epoch 9/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 4.5511e-05\n",
            "Epoch 10/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 4.3440e-05\n",
            "Epoch 11/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 4.2307e-05\n",
            "Epoch 12/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 4.5675e-05\n",
            "Epoch 13/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 4.4213e-05\n",
            "Epoch 14/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 3.6071e-05\n",
            "Epoch 15/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 4.7469e-05\n",
            "Epoch 16/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 4.3619e-05\n",
            "Epoch 17/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 3.8887e-05\n",
            "Epoch 18/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 4.0500e-05\n",
            "Epoch 19/20\n",
            "3277/3277 [==============================] - 33s 10ms/step - loss: 3.8383e-05\n",
            "Epoch 20/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 3.6284e-05\n",
            "\n",
            "NVDA -\n",
            "Epoch 1/20\n",
            "3277/3277 [==============================] - 35s 9ms/step - loss: 6.6107e-05\n",
            "Epoch 2/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 2.5784e-05\n",
            "Epoch 3/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 1.5621e-05\n",
            "Epoch 4/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 1.6703e-05\n",
            "Epoch 5/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 1.2346e-05\n",
            "Epoch 6/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 1.0141e-05\n",
            "Epoch 7/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 1.0601e-05\n",
            "Epoch 8/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 1.0331e-05\n",
            "Epoch 9/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 8.5226e-06\n",
            "Epoch 10/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 9.1265e-06\n",
            "Epoch 11/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 8.4084e-06\n",
            "Epoch 12/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 8.7531e-06\n",
            "Epoch 13/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 7.7543e-06\n",
            "Epoch 14/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 7.7010e-06\n",
            "Epoch 15/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 7.4845e-06\n",
            "Epoch 16/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 7.7351e-06\n",
            "Epoch 17/20\n",
            "3277/3277 [==============================] - 31s 10ms/step - loss: 7.6011e-06\n",
            "Epoch 18/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 7.3578e-06\n",
            "Epoch 19/20\n",
            "3277/3277 [==============================] - 32s 10ms/step - loss: 6.5572e-06\n",
            "Epoch 20/20\n",
            "3277/3277 [==============================] - 31s 9ms/step - loss: 7.5866e-06\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_dict = {}\n",
        "\n",
        "for stock in x_train_dict:\n",
        "\n",
        "  print(stock, \"-\")\n",
        "\n",
        "  #now we are Building the LSTM network model\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(units=50, return_sequences=True,input_shape=(x_train_dict[\"AAPL\"].shape[1],1)))\n",
        "  model.add(LSTM(units=50, return_sequences=False))\n",
        "  model.add(Dense(units=25))\n",
        "  model.add(Dense(units=1))\n",
        "\n",
        "  if stock == \"AAPL\":\n",
        "    print(model.summary())\n",
        "\n",
        "  # here we are Compiling the model\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "  # here we are training the model\n",
        "  model.fit(x_train_dict[stock], y_train_dict[stock], batch_size=1, epochs=20)\n",
        "\n",
        "  # Store the trained model in the model dictionary\n",
        "  model_dict[stock] = model\n",
        "\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zwrwp0RaS-zJ"
      },
      "outputs": [],
      "source": [
        "# Saving all the models\n",
        "for stock in model_dict:\n",
        "  filename = \"keras_model_\" + stock + \".h5\"\n",
        "  model_dict[stock].save(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMNBLfikYdfb"
      },
      "outputs": [],
      "source": [
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enTiw4YYoUL2"
      },
      "source": [
        "## Case 2: Small Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsxtCdpwo1j0"
      },
      "source": [
        "### Importing dataset from yahoo finance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH3tQ9ryDyAm"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhrTgJucomZp"
      },
      "outputs": [],
      "source": [
        "start = \"2023-01-01\"\n",
        "end = \"2023-07-31\"\n",
        "\n",
        "# Convert start and end dates to datetime objects\n",
        "start_date = datetime.strptime(start, \"%Y-%m-%d\")\n",
        "end_date = datetime.strptime(end, \"%Y-%m-%d\")\n",
        "\n",
        "# Set up the data reader with Yahoo Finance\n",
        "yf.pdr_override()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Znr6ox7qoXBD"
      },
      "outputs": [],
      "source": [
        "# Making dictionary for every stock\n",
        "stock_dict_small = {}\n",
        "for stock in stock_types:\n",
        "  stock_dict_small[stock] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdFE7IhKoeeI",
        "outputId": "b733e513-6efb-4945-8f99-90761a5e08b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "AAPL-\n",
            "        Date        Open        High         Low       Close   Adj Close  \\\n",
            "0 2023-01-03  130.279999  130.899994  124.169998  125.070000  124.538658   \n",
            "1 2023-01-04  126.889999  128.660004  125.080002  126.360001  125.823189   \n",
            "2 2023-01-05  127.129997  127.769997  124.760002  125.019997  124.488869   \n",
            "3 2023-01-06  126.010002  130.289993  124.889999  129.619995  129.069336   \n",
            "4 2023-01-09  130.470001  133.410004  129.889999  130.149994  129.597076   \n",
            "\n",
            "      Volume  \n",
            "0  112117500  \n",
            "1   89113600  \n",
            "2   80962700  \n",
            "3   87754700  \n",
            "4   70790800  \n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "MSFT-\n",
            "        Date        Open        High         Low       Close   Adj Close  \\\n",
            "0 2023-01-03  243.080002  245.750000  237.399994  239.580002  238.460129   \n",
            "1 2023-01-04  232.279999  232.869995  225.960007  229.100006  228.029114   \n",
            "2 2023-01-05  227.199997  227.550003  221.759995  222.309998  221.270859   \n",
            "3 2023-01-06  223.000000  225.759995  219.350006  224.929993  223.878616   \n",
            "4 2023-01-09  226.449997  231.240005  226.410004  227.119995  226.058365   \n",
            "\n",
            "     Volume  \n",
            "0  25740000  \n",
            "1  50623400  \n",
            "2  39585600  \n",
            "3  43613600  \n",
            "4  27369800  \n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "AMZN-\n",
            "        Date       Open       High        Low      Close  Adj Close    Volume\n",
            "0 2023-01-03  85.459999  86.959999  84.209999  85.820000  85.820000  76706000\n",
            "1 2023-01-04  86.550003  86.980003  83.360001  85.139999  85.139999  68885100\n",
            "2 2023-01-05  85.330002  85.419998  83.070000  83.120003  83.120003  67930800\n",
            "3 2023-01-06  83.029999  86.400002  81.430000  86.080002  86.080002  83303400\n",
            "4 2023-01-09  87.459999  89.480003  87.080002  87.360001  87.360001  65266100\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "NFLX-\n",
            "        Date        Open        High         Low       Close   Adj Close  \\\n",
            "0 2023-01-03  298.059998  298.390015  288.700012  294.950012  294.950012   \n",
            "1 2023-01-04  298.239990  311.140015  295.510010  309.410004  309.410004   \n",
            "2 2023-01-05  307.000000  314.179993  304.549988  309.700012  309.700012   \n",
            "3 2023-01-06  311.570007  316.769989  303.690002  315.549988  315.549988   \n",
            "4 2023-01-09  316.829987  321.700012  313.220001  315.170013  315.170013   \n",
            "\n",
            "    Volume  \n",
            "0  6764000  \n",
            "1  9345100  \n",
            "2  8328400  \n",
            "3  8959800  \n",
            "4  6766600  \n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "INFY-\n",
            "        Date       Open       High        Low      Close  Adj Close    Volume\n",
            "0 2023-01-03  18.180000  18.350000  18.059999  18.230000  17.986706  11041400\n",
            "1 2023-01-04  17.940001  18.170000  17.860001  18.100000  17.858440  10440400\n",
            "2 2023-01-05  17.809999  17.870001  17.680000  17.680000  17.444046   6420300\n",
            "3 2023-01-06  17.530001  17.840000  17.469999  17.809999  17.572309   8915600\n",
            "4 2023-01-09  18.090000  18.240000  17.990000  18.000000  17.759775  10778100\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "ADBE-\n",
            "        Date        Open        High         Low       Close   Adj Close  \\\n",
            "0 2023-01-03  340.160004  345.820007  331.920013  336.920013  336.920013   \n",
            "1 2023-01-04  342.890015  345.630005  336.730011  341.410004  341.410004   \n",
            "2 2023-01-05  337.529999  337.549988  328.160004  328.440002  328.440002   \n",
            "3 2023-01-06  332.279999  334.880005  322.440002  332.750000  332.750000   \n",
            "4 2023-01-09  338.899994  349.459991  338.420013  341.980011  341.980011   \n",
            "\n",
            "    Volume  \n",
            "0  2229100  \n",
            "1  2186800  \n",
            "2  2081600  \n",
            "3  2216600  \n",
            "4  3133800  \n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "GOOGL-\n",
            "        Date       Open       High        Low      Close  Adj Close    Volume\n",
            "0 2023-01-03  89.589996  91.050003  88.519997  89.120003  89.120003  28131200\n",
            "1 2023-01-04  90.349998  90.650002  87.269997  88.080002  88.080002  34854800\n",
            "2 2023-01-05  87.470001  87.570000  85.900002  86.199997  86.199997  27194400\n",
            "3 2023-01-06  86.790001  87.690002  84.860001  87.339996  87.339996  41381500\n",
            "4 2023-01-09  88.360001  90.050003  87.860001  88.019997  88.019997  29003900\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "NVDA-\n",
            "        Date        Open        High         Low       Close   Adj Close  \\\n",
            "0 2023-01-03  148.509995  149.960007  140.960007  143.149994  143.110870   \n",
            "1 2023-01-04  145.669998  148.529999  142.410004  147.490005  147.449707   \n",
            "2 2023-01-05  144.910004  145.639999  141.479996  142.649994  142.611023   \n",
            "3 2023-01-06  144.740005  150.100006  140.339996  148.589996  148.549393   \n",
            "4 2023-01-09  152.839996  160.559998  151.410004  156.279999  156.237289   \n",
            "\n",
            "     Volume  \n",
            "0  40127700  \n",
            "1  43132400  \n",
            "2  38916800  \n",
            "3  40504400  \n",
            "4  50423100  \n"
          ]
        }
      ],
      "source": [
        "for stock in stock_dict_small:\n",
        "  # Fetch the data using DataReader\n",
        "  df = pdr.get_data_yahoo(stock, start=start_date, end=end_date)\n",
        "  df = df.reset_index()\n",
        "\n",
        "  if df is not None:\n",
        "    stock_dict_small[stock] = df\n",
        "    print(stock + \"-\" )\n",
        "    print(df.head())  # Display the first few rows of the loaded data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk6LNm87o827",
        "outputId": "d4ea01e2-3031-4572-b961-c5d8cdacce9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AAPL-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 143 entries, 0 to 142\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       143 non-null    datetime64[ns]\n",
            " 1   Open       143 non-null    float64       \n",
            " 2   High       143 non-null    float64       \n",
            " 3   Low        143 non-null    float64       \n",
            " 4   Close      143 non-null    float64       \n",
            " 5   Adj Close  143 non-null    float64       \n",
            " 6   Volume     143 non-null    int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 7.9 KB\n",
            "None\n",
            "             Open        High         Low       Close   Adj Close  \\\n",
            "count  143.000000  143.000000  143.000000  143.000000  143.000000   \n",
            "mean   164.616504  166.341399  163.453497  165.059231  164.663173   \n",
            "std     18.615219   18.412319   18.713290   18.486263   18.591757   \n",
            "min    126.010002  127.769997  124.169998  125.019997  124.488869   \n",
            "25%    150.794998  153.195000  150.010002  151.315002  150.851700   \n",
            "50%    164.889999  166.289993  163.889999  165.229996  164.779251   \n",
            "75%    180.500000  181.495003  177.750000  180.330002  180.086815   \n",
            "max    196.020004  198.229996  194.139999  195.830002  195.565918   \n",
            "\n",
            "             Volume  \n",
            "count  1.430000e+02  \n",
            "mean   6.133096e+07  \n",
            "std    1.812899e+07  \n",
            "min    3.145820e+07  \n",
            "25%    4.912860e+07  \n",
            "50%    5.605830e+07  \n",
            "75%    6.866110e+07  \n",
            "max    1.543573e+08  \n",
            "MSFT-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 143 entries, 0 to 142\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       143 non-null    datetime64[ns]\n",
            " 1   Open       143 non-null    float64       \n",
            " 2   High       143 non-null    float64       \n",
            " 3   Low        143 non-null    float64       \n",
            " 4   Close      143 non-null    float64       \n",
            " 5   Adj Close  143 non-null    float64       \n",
            " 6   Volume     143 non-null    int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 7.9 KB\n",
            "None\n",
            "             Open        High         Low       Close   Adj Close  \\\n",
            "count  143.000000  143.000000  143.000000  143.000000  143.000000   \n",
            "mean   291.502378  294.795734  288.669860  291.917272  291.407241   \n",
            "std     37.782455   38.060312   37.434179   37.681758   38.050418   \n",
            "min    223.000000  225.759995  219.350006  222.309998  221.270859   \n",
            "25%    256.589996  259.824997  255.320000  256.819992  255.939735   \n",
            "50%    285.989990  289.269989  283.950012  288.299988  287.671112   \n",
            "75%    331.754990  334.860001  327.164993  331.519989  331.519989   \n",
            "max    361.750000  366.779999  352.440002  359.489990  359.489990   \n",
            "\n",
            "             Volume  \n",
            "count  1.430000e+02  \n",
            "mean   3.056305e+07  \n",
            "std    1.096862e+07  \n",
            "min    1.250870e+07  \n",
            "25%    2.333130e+07  \n",
            "50%    2.727600e+07  \n",
            "75%    3.369935e+07  \n",
            "max    6.952740e+07  \n",
            "AMZN-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 143 entries, 0 to 142\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       143 non-null    datetime64[ns]\n",
            " 1   Open       143 non-null    float64       \n",
            " 2   High       143 non-null    float64       \n",
            " 3   Low        143 non-null    float64       \n",
            " 4   Close      143 non-null    float64       \n",
            " 5   Adj Close  143 non-null    float64       \n",
            " 6   Volume     143 non-null    int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 7.9 KB\n",
            "None\n",
            "             Open        High         Low       Close   Adj Close  \\\n",
            "count  143.000000  143.000000  143.000000  143.000000  143.000000   \n",
            "mean   108.550560  110.156783  107.160490  108.761888  108.761888   \n",
            "std     14.236564   14.227120   14.229355   14.112983   14.112983   \n",
            "min     83.029999   85.419998   81.430000   83.120003   83.120003   \n",
            "25%     97.959999   98.855000   96.115002   97.770000   97.770000   \n",
            "50%    103.529999  105.120003  101.820000  103.389999  103.389999   \n",
            "75%    124.049999  126.095001  123.110001  124.250000  124.250000   \n",
            "max    134.559998  136.649994  134.059998  135.360001  135.360001   \n",
            "\n",
            "             Volume  \n",
            "count  1.430000e+02  \n",
            "mean   6.401132e+07  \n",
            "std    2.124589e+07  \n",
            "min    2.826480e+07  \n",
            "25%    5.139425e+07  \n",
            "50%    5.859730e+07  \n",
            "75%    7.110700e+07  \n",
            "max    1.581542e+08  \n",
            "NFLX-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 143 entries, 0 to 142\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       143 non-null    datetime64[ns]\n",
            " 1   Open       143 non-null    float64       \n",
            " 2   High       143 non-null    float64       \n",
            " 3   Low        143 non-null    float64       \n",
            " 4   Close      143 non-null    float64       \n",
            " 5   Adj Close  143 non-null    float64       \n",
            " 6   Volume     143 non-null    int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 7.9 KB\n",
            "None\n",
            "             Open        High         Low       Close   Adj Close  \\\n",
            "count  143.000000  143.000000  143.000000  143.000000  143.000000   \n",
            "mean   361.028880  366.766504  355.891399  361.687902  361.687902   \n",
            "std     47.795889   48.281851   47.363602   47.899846   47.899846   \n",
            "min    287.339996  297.450012  285.329987  292.760010  292.760010   \n",
            "25%    324.254990  329.675003  320.550003  324.735001  324.735001   \n",
            "50%    343.450012  348.170013  338.750000  346.190002  346.190002   \n",
            "75%    403.945007  411.699997  398.080002  403.335007  403.335007   \n",
            "max    476.859985  485.000000  470.000000  477.589996  477.589996   \n",
            "\n",
            "             Volume  \n",
            "count  1.430000e+02  \n",
            "mean   7.483682e+06  \n",
            "std    4.205454e+06  \n",
            "min    2.657900e+06  \n",
            "25%    4.965150e+06  \n",
            "50%    6.287300e+06  \n",
            "75%    8.145150e+06  \n",
            "max    2.843030e+07  \n",
            "INFY-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 143 entries, 0 to 142\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       143 non-null    datetime64[ns]\n",
            " 1   Open       143 non-null    float64       \n",
            " 2   High       143 non-null    float64       \n",
            " 3   Low        143 non-null    float64       \n",
            " 4   Close      143 non-null    float64       \n",
            " 5   Adj Close  143 non-null    float64       \n",
            " 6   Volume     143 non-null    int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 7.9 KB\n",
            "None\n",
            "             Open        High         Low       Close   Adj Close  \\\n",
            "count  143.000000  143.000000  143.000000  143.000000  143.000000   \n",
            "mean    16.849021   16.978741   16.730350   16.866713   16.701603   \n",
            "std      1.401260    1.414323    1.396801    1.404612    1.355618   \n",
            "min     14.820000   14.860000   14.710000   14.790000   14.592615   \n",
            "25%     15.585000   15.665000   15.420000   15.560000   15.460502   \n",
            "50%     16.770000   16.910000   16.650000   16.790001   16.629999   \n",
            "75%     18.115000   18.205000   17.960000   18.140000   17.898819   \n",
            "max     19.520000   19.590000   19.299999   19.510000   19.249622   \n",
            "\n",
            "             Volume  \n",
            "count  1.430000e+02  \n",
            "mean   1.020244e+07  \n",
            "std    5.303912e+06  \n",
            "min    4.293700e+06  \n",
            "25%    7.074200e+06  \n",
            "50%    8.848300e+06  \n",
            "75%    1.174565e+07  \n",
            "max    4.041500e+07  \n",
            "ADBE-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 143 entries, 0 to 142\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       143 non-null    datetime64[ns]\n",
            " 1   Open       143 non-null    float64       \n",
            " 2   High       143 non-null    float64       \n",
            " 3   Low        143 non-null    float64       \n",
            " 4   Close      143 non-null    float64       \n",
            " 5   Adj Close  143 non-null    float64       \n",
            " 6   Volume     143 non-null    int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 7.9 KB\n",
            "None\n",
            "             Open        High         Low       Close   Adj Close  \\\n",
            "count  143.000000  143.000000  143.000000  143.000000  143.000000   \n",
            "mean   395.672028  401.361889  391.178113  396.618880  396.618880   \n",
            "std     61.326142   62.494880   60.916349   61.815628   61.815628   \n",
            "min    322.019989  325.100006  318.600006  320.540009  320.540009   \n",
            "25%    347.914993  353.210007  343.994995  347.019989  347.019989   \n",
            "50%    374.010010  377.690002  369.679993  374.959991  374.959991   \n",
            "75%    432.779999  439.455002  428.659988  435.274994  435.274994   \n",
            "max    534.479980  539.000000  526.059998  532.229980  532.229980   \n",
            "\n",
            "             Volume  \n",
            "count  1.430000e+02  \n",
            "mean   3.359004e+06  \n",
            "std    1.794327e+06  \n",
            "min    1.491000e+06  \n",
            "25%    2.282400e+06  \n",
            "50%    2.803200e+06  \n",
            "75%    3.822050e+06  \n",
            "max    1.333580e+07  \n",
            "GOOGL-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 143 entries, 0 to 142\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       143 non-null    datetime64[ns]\n",
            " 1   Open       143 non-null    float64       \n",
            " 2   High       143 non-null    float64       \n",
            " 3   Low        143 non-null    float64       \n",
            " 4   Close      143 non-null    float64       \n",
            " 5   Adj Close  143 non-null    float64       \n",
            " 6   Volume     143 non-null    int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 7.9 KB\n",
            "None\n",
            "             Open        High         Low       Close   Adj Close  \\\n",
            "count  143.000000  143.000000  143.000000  143.000000  143.000000   \n",
            "mean   107.461259  109.036644  106.351678  107.782168  107.782168   \n",
            "std     12.704584   12.799080   12.610591   12.620808   12.620808   \n",
            "min     85.980003   87.570000   84.860001   86.199997   86.199997   \n",
            "25%     95.509998   97.625000   94.494999   96.525002   96.525002   \n",
            "50%    105.470001  106.690002  104.110001  105.410004  105.410004   \n",
            "75%    120.690002  122.305000  119.315002  120.695000  120.695000   \n",
            "max    131.669998  133.740005  130.570007  132.580002  132.580002   \n",
            "\n",
            "             Volume  \n",
            "count  1.430000e+02  \n",
            "mean   3.669305e+07  \n",
            "std    1.415642e+07  \n",
            "min    1.446790e+07  \n",
            "25%    2.770280e+07  \n",
            "50%    3.326680e+07  \n",
            "75%    4.093580e+07  \n",
            "max    1.194550e+08  \n",
            "NVDA-\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 143 entries, 0 to 142\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Date       143 non-null    datetime64[ns]\n",
            " 1   Open       143 non-null    float64       \n",
            " 2   High       143 non-null    float64       \n",
            " 3   Low        143 non-null    float64       \n",
            " 4   Close      143 non-null    float64       \n",
            " 5   Adj Close  143 non-null    float64       \n",
            " 6   Volume     143 non-null    int64         \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 7.9 KB\n",
            "None\n",
            "             Open        High         Low       Close   Adj Close  \\\n",
            "count  143.000000  143.000000  143.000000  143.000000  143.000000   \n",
            "mean   296.031957  301.689999  291.456292  297.112867  297.083162   \n",
            "std     94.093233   94.999015   92.515861   93.267169   93.284159   \n",
            "min    144.740005  145.639999  140.339996  142.649994  142.611023   \n",
            "25%    226.790001  232.769997  223.645004  229.654999  229.631241   \n",
            "50%    272.250000  275.890015  267.220001  271.910004  271.881866   \n",
            "75%    391.184998  398.804993  385.925003  392.489990  392.449387   \n",
            "max    474.640015  480.880005  467.420013  474.940002  474.940002   \n",
            "\n",
            "             Volume  \n",
            "count  1.430000e+02  \n",
            "mean   4.840586e+07  \n",
            "std    1.702760e+07  \n",
            "min    1.982090e+07  \n",
            "25%    3.829515e+07  \n",
            "50%    4.494050e+07  \n",
            "75%    5.400035e+07  \n",
            "max    1.543911e+08  \n"
          ]
        }
      ],
      "source": [
        "for stock in stock_dict:\n",
        "  print(stock + \"-\")\n",
        "  print(stock_dict_small[stock].info())\n",
        "  print(stock_dict_small[stock].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIfcXE2RpIwC"
      },
      "outputs": [],
      "source": [
        "dataset_small = {}\n",
        "\n",
        "for stock in stock_dict_small:\n",
        "\n",
        "  #Creating a new dataframe with only the 'Close' column\n",
        "  data = stock_dict_small[stock].filter(['Close'])\n",
        "\n",
        "  #Converting the dataframe to a numpy array\n",
        "  dataset_small[stock] = data.values\n",
        "  dataset_small[stock] = dataset_small[stock].reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMUik3x2pbAA",
        "outputId": "bfa16386-9932-4078-9286-c6b41223e697"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "115"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Get /Compute the number of rows to train the model on\n",
        "training_data_len_small = math.ceil( len(dataset_small[stock]) *.8)\n",
        "training_data_len_small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkRbQ_xfpk2m"
      },
      "outputs": [],
      "source": [
        "scaled_data_dict_small = {}\n",
        "\n",
        "for stock in dataset_small:\n",
        "  # here we are Scaling the all of the data to be values between 0 and 1\n",
        "  scaled_data_dict_small[stock] = scaler.fit_transform(dataset_small[stock])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af0J1dWbpzq_"
      },
      "outputs": [],
      "source": [
        "train_data_dict_small = {}\n",
        "x_train_dict_small = {}\n",
        "y_train_dict_small = {}\n",
        "\n",
        "for stock in scaled_data_dict_small:\n",
        "  #Creating the scaled training data set\n",
        "  train_data_dict_small[stock] = scaled_data_dict_small[stock][0:training_data_len_small  , : ]\n",
        "  #Spliting the data into x_train and y_train data sets\n",
        "  x_train=[]\n",
        "  y_train = []\n",
        "  for i in range(20,len(train_data_dict_small[stock])):\n",
        "    x_train.append(train_data_dict_small[stock][i-20:i,0])\n",
        "    y_train.append(train_data_dict_small[stock][i,0])\n",
        "\n",
        "  #Here we are Converting x_train and y_train to numpy arrays\n",
        "  x_train_dict_small[stock] = np.array(x_train)\n",
        "  y_train_dict_small[stock] = np.array(y_train)\n",
        "\n",
        "  # Here we are reshaping the data into the shape accepted by the LSTM\n",
        "  x_train_dict_small[stock] = np.reshape(x_train_dict_small[stock], (x_train_dict_small[stock].shape[0], x_train_dict_small[stock].shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5fxMt6qqxJ8",
        "outputId": "3c48402a-2bb9-4ec7-91f0-fa96e00b220c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AAPL -\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_14 (LSTM)              (None, 60, 50)            10400     \n",
            "                                                                 \n",
            " lstm_15 (LSTM)              (None, 50)                20200     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 25)                1275      \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1)                 26        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,901\n",
            "Trainable params: 31,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "95/95 [==============================] - 5s 9ms/step - loss: 0.0211\n",
            "Epoch 2/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0046\n",
            "Epoch 3/50\n",
            "95/95 [==============================] - 1s 11ms/step - loss: 0.0031\n",
            "Epoch 4/50\n",
            "95/95 [==============================] - 1s 13ms/step - loss: 0.0035\n",
            "Epoch 5/50\n",
            "95/95 [==============================] - 1s 13ms/step - loss: 0.0034\n",
            "Epoch 6/50\n",
            "95/95 [==============================] - 1s 11ms/step - loss: 0.0027\n",
            "Epoch 7/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0027\n",
            "Epoch 8/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0027\n",
            "Epoch 9/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0029\n",
            "Epoch 10/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0028\n",
            "Epoch 11/50\n",
            "95/95 [==============================] - 1s 11ms/step - loss: 0.0029\n",
            "Epoch 12/50\n",
            "95/95 [==============================] - 1s 10ms/step - loss: 0.0025\n",
            "Epoch 13/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0027\n",
            "Epoch 14/50\n",
            "95/95 [==============================] - 1s 10ms/step - loss: 0.0030\n",
            "Epoch 15/50\n",
            "95/95 [==============================] - 2s 18ms/step - loss: 0.0025\n",
            "Epoch 16/50\n",
            "95/95 [==============================] - 2s 19ms/step - loss: 0.0027\n",
            "Epoch 17/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0029\n",
            "Epoch 18/50\n",
            "95/95 [==============================] - 1s 10ms/step - loss: 0.0019\n",
            "Epoch 19/50\n",
            "95/95 [==============================] - 1s 10ms/step - loss: 0.0042\n",
            "Epoch 20/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 21/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0017\n",
            "Epoch 22/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0018\n",
            "Epoch 23/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 24/50\n",
            "95/95 [==============================] - 1s 6ms/step - loss: 0.0024\n",
            "Epoch 25/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0024\n",
            "Epoch 26/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0021\n",
            "Epoch 27/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0017\n",
            "Epoch 28/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0018\n",
            "Epoch 29/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0018\n",
            "Epoch 30/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0020\n",
            "Epoch 31/50\n",
            "95/95 [==============================] - 1s 10ms/step - loss: 0.0019\n",
            "Epoch 32/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0017\n",
            "Epoch 33/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0017\n",
            "Epoch 34/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0016\n",
            "Epoch 35/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0017\n",
            "Epoch 36/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0017\n",
            "Epoch 37/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 38/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0017\n",
            "Epoch 39/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0013\n",
            "Epoch 40/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 41/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0014\n",
            "Epoch 42/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 43/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0013\n",
            "Epoch 44/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0012\n",
            "Epoch 45/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0014\n",
            "Epoch 46/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 47/50\n",
            "95/95 [==============================] - 1s 10ms/step - loss: 0.0012\n",
            "Epoch 48/50\n",
            "95/95 [==============================] - 1s 11ms/step - loss: 0.0014\n",
            "Epoch 49/50\n",
            "95/95 [==============================] - 1s 12ms/step - loss: 0.0012\n",
            "Epoch 50/50\n",
            "95/95 [==============================] - 1s 15ms/step - loss: 0.0015\n",
            "\n",
            "MSFT -\n",
            "Epoch 1/50\n",
            "95/95 [==============================] - 5s 7ms/step - loss: 0.0220\n",
            "Epoch 2/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0051\n",
            "Epoch 3/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0058\n",
            "Epoch 4/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0054\n",
            "Epoch 5/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0043\n",
            "Epoch 6/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0057\n",
            "Epoch 7/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0037\n",
            "Epoch 8/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0039\n",
            "Epoch 9/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0043\n",
            "Epoch 10/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0040\n",
            "Epoch 11/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0034\n",
            "Epoch 12/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0028\n",
            "Epoch 13/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0028\n",
            "Epoch 14/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0039\n",
            "Epoch 15/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0033\n",
            "Epoch 16/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0032\n",
            "Epoch 17/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0032\n",
            "Epoch 18/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0029\n",
            "Epoch 19/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0027\n",
            "Epoch 20/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0024\n",
            "Epoch 21/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0026\n",
            "Epoch 22/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0024\n",
            "Epoch 23/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0025\n",
            "Epoch 24/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0026\n",
            "Epoch 25/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0023\n",
            "Epoch 26/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0023\n",
            "Epoch 27/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0028\n",
            "Epoch 28/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0024\n",
            "Epoch 29/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 30/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0020\n",
            "Epoch 31/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0018\n",
            "Epoch 32/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0023\n",
            "Epoch 33/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0022\n",
            "Epoch 34/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0028\n",
            "Epoch 35/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0018\n",
            "Epoch 36/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0016\n",
            "Epoch 37/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0016\n",
            "Epoch 38/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 39/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0023\n",
            "Epoch 40/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0016\n",
            "Epoch 41/50\n",
            "95/95 [==============================] - 1s 10ms/step - loss: 0.0015\n",
            "Epoch 42/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0019\n",
            "Epoch 43/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 44/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0019\n",
            "Epoch 45/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0017\n",
            "Epoch 46/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0022\n",
            "Epoch 47/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 48/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0016\n",
            "Epoch 49/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 50/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0018\n",
            "\n",
            "AMZN -\n",
            "Epoch 1/50\n",
            "95/95 [==============================] - 5s 7ms/step - loss: 0.0171\n",
            "Epoch 2/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0058\n",
            "Epoch 3/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0060\n",
            "Epoch 4/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0053\n",
            "Epoch 5/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0055\n",
            "Epoch 6/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0046\n",
            "Epoch 7/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0043\n",
            "Epoch 8/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0036\n",
            "Epoch 9/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0042\n",
            "Epoch 10/50\n",
            "95/95 [==============================] - 1s 6ms/step - loss: 0.0040\n",
            "Epoch 11/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0041\n",
            "Epoch 12/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0039\n",
            "Epoch 13/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0052\n",
            "Epoch 14/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0035\n",
            "Epoch 15/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0032\n",
            "Epoch 16/50\n",
            "95/95 [==============================] - 1s 10ms/step - loss: 0.0035\n",
            "Epoch 17/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0037\n",
            "Epoch 18/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0042\n",
            "Epoch 19/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0035\n",
            "Epoch 20/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0045\n",
            "Epoch 21/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0031\n",
            "Epoch 22/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0026\n",
            "Epoch 23/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0029\n",
            "Epoch 24/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0035\n",
            "Epoch 25/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0033\n",
            "Epoch 26/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0029\n",
            "Epoch 27/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0029\n",
            "Epoch 28/50\n",
            "95/95 [==============================] - 1s 13ms/step - loss: 0.0031\n",
            "Epoch 29/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0030\n",
            "Epoch 30/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0030\n",
            "Epoch 31/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0035\n",
            "Epoch 32/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0027\n",
            "Epoch 33/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0025\n",
            "Epoch 34/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0025\n",
            "Epoch 35/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0027\n",
            "Epoch 36/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0027\n",
            "Epoch 37/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0026\n",
            "Epoch 38/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0029\n",
            "Epoch 39/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0036\n",
            "Epoch 40/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0033\n",
            "Epoch 41/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0027\n",
            "Epoch 42/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0025\n",
            "Epoch 43/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0028\n",
            "Epoch 44/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0025\n",
            "Epoch 45/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0026\n",
            "Epoch 46/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0027\n",
            "Epoch 47/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0023\n",
            "Epoch 48/50\n",
            "95/95 [==============================] - 1s 11ms/step - loss: 0.0028\n",
            "Epoch 49/50\n",
            "95/95 [==============================] - 2s 17ms/step - loss: 0.0032\n",
            "Epoch 50/50\n",
            "95/95 [==============================] - 2s 24ms/step - loss: 0.0027\n",
            "\n",
            "NFLX -\n",
            "Epoch 1/50\n",
            "95/95 [==============================] - 4s 7ms/step - loss: 0.0256\n",
            "Epoch 2/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0094\n",
            "Epoch 3/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0074\n",
            "Epoch 4/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0067\n",
            "Epoch 5/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0039\n",
            "Epoch 6/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0035\n",
            "Epoch 7/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0037\n",
            "Epoch 8/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0047\n",
            "Epoch 9/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0036\n",
            "Epoch 10/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0036\n",
            "Epoch 11/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0041\n",
            "Epoch 12/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0044\n",
            "Epoch 13/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0033\n",
            "Epoch 14/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0030\n",
            "Epoch 15/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0030\n",
            "Epoch 16/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0032\n",
            "Epoch 17/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0033\n",
            "Epoch 18/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0029\n",
            "Epoch 19/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0049\n",
            "Epoch 20/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0034\n",
            "Epoch 21/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0035\n",
            "Epoch 22/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0032\n",
            "Epoch 23/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0036\n",
            "Epoch 24/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0027\n",
            "Epoch 25/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0033\n",
            "Epoch 26/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0034\n",
            "Epoch 27/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0030\n",
            "Epoch 28/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0028\n",
            "Epoch 29/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0035\n",
            "Epoch 30/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0030\n",
            "Epoch 31/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0028\n",
            "Epoch 32/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0032\n",
            "Epoch 33/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0026\n",
            "Epoch 34/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0030\n",
            "Epoch 35/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0034\n",
            "Epoch 36/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0029\n",
            "Epoch 37/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0028\n",
            "Epoch 38/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0024\n",
            "Epoch 39/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0027\n",
            "Epoch 40/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0031\n",
            "Epoch 41/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0022\n",
            "Epoch 42/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0025\n",
            "Epoch 43/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0022\n",
            "Epoch 44/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0018\n",
            "Epoch 45/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0028\n",
            "Epoch 46/50\n",
            "95/95 [==============================] - 1s 12ms/step - loss: 0.0020\n",
            "Epoch 47/50\n",
            "95/95 [==============================] - 2s 22ms/step - loss: 0.0020\n",
            "Epoch 48/50\n",
            "95/95 [==============================] - 2s 18ms/step - loss: 0.0022\n",
            "Epoch 49/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 50/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0030\n",
            "\n",
            "INFY -\n",
            "Epoch 1/50\n",
            "95/95 [==============================] - 4s 7ms/step - loss: 0.0394\n",
            "Epoch 2/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0147\n",
            "Epoch 3/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0125\n",
            "Epoch 4/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0139\n",
            "Epoch 5/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0105\n",
            "Epoch 6/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0101\n",
            "Epoch 7/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0076\n",
            "Epoch 8/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0086\n",
            "Epoch 9/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0060\n",
            "Epoch 10/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0055\n",
            "Epoch 11/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0066\n",
            "Epoch 12/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0049\n",
            "Epoch 13/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0062\n",
            "Epoch 14/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0069\n",
            "Epoch 15/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0052\n",
            "Epoch 16/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0045\n",
            "Epoch 17/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0050\n",
            "Epoch 18/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0044\n",
            "Epoch 19/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0052\n",
            "Epoch 20/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0056\n",
            "Epoch 21/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0044\n",
            "Epoch 22/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0056\n",
            "Epoch 23/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0042\n",
            "Epoch 24/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0042\n",
            "Epoch 25/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0045\n",
            "Epoch 26/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0041\n",
            "Epoch 27/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0043\n",
            "Epoch 28/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0040\n",
            "Epoch 29/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0039\n",
            "Epoch 30/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0040\n",
            "Epoch 31/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0043\n",
            "Epoch 32/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0036\n",
            "Epoch 33/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0044\n",
            "Epoch 34/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0038\n",
            "Epoch 35/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0037\n",
            "Epoch 36/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0050\n",
            "Epoch 37/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0046\n",
            "Epoch 38/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0044\n",
            "Epoch 39/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0037\n",
            "Epoch 40/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0042\n",
            "Epoch 41/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0040\n",
            "Epoch 42/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0034\n",
            "Epoch 43/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0046\n",
            "Epoch 44/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0041\n",
            "Epoch 45/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0040\n",
            "Epoch 46/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0035\n",
            "Epoch 47/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0043\n",
            "Epoch 48/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0036\n",
            "Epoch 49/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0044\n",
            "Epoch 50/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0041\n",
            "\n",
            "ADBE -\n",
            "Epoch 1/50\n",
            "95/95 [==============================] - 5s 9ms/step - loss: 0.0168\n",
            "Epoch 2/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0079\n",
            "Epoch 3/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0069\n",
            "Epoch 4/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0051\n",
            "Epoch 5/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0058\n",
            "Epoch 6/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0045\n",
            "Epoch 7/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0045\n",
            "Epoch 8/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0045\n",
            "Epoch 9/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0056\n",
            "Epoch 10/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0041\n",
            "Epoch 11/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0039\n",
            "Epoch 12/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0041\n",
            "Epoch 13/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0040\n",
            "Epoch 14/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0045\n",
            "Epoch 15/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0047\n",
            "Epoch 16/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0040\n",
            "Epoch 17/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0038\n",
            "Epoch 18/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0048\n",
            "Epoch 19/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0036\n",
            "Epoch 20/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0030\n",
            "Epoch 21/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0035\n",
            "Epoch 22/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0037\n",
            "Epoch 23/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0030\n",
            "Epoch 24/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0027\n",
            "Epoch 25/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0028\n",
            "Epoch 26/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0026\n",
            "Epoch 27/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0023\n",
            "Epoch 28/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0025\n",
            "Epoch 29/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0026\n",
            "Epoch 30/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0021\n",
            "Epoch 31/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0020\n",
            "Epoch 32/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0020\n",
            "Epoch 33/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0018\n",
            "Epoch 34/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 35/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0019\n",
            "Epoch 36/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0022\n",
            "Epoch 37/50\n",
            "95/95 [==============================] - 1s 10ms/step - loss: 0.0019\n",
            "Epoch 38/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0021\n",
            "Epoch 39/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0024\n",
            "Epoch 40/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0020\n",
            "Epoch 41/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0018\n",
            "Epoch 42/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 43/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0020\n",
            "Epoch 44/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0022\n",
            "Epoch 45/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0026\n",
            "Epoch 46/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0023\n",
            "Epoch 47/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0018\n",
            "Epoch 48/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 49/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0031\n",
            "Epoch 50/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0021\n",
            "\n",
            "GOOGL -\n",
            "Epoch 1/50\n",
            "95/95 [==============================] - 4s 7ms/step - loss: 0.0248\n",
            "Epoch 2/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0111\n",
            "Epoch 3/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0111\n",
            "Epoch 4/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0108\n",
            "Epoch 5/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0075\n",
            "Epoch 6/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0066\n",
            "Epoch 7/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0068\n",
            "Epoch 8/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0062\n",
            "Epoch 9/50\n",
            "95/95 [==============================] - 1s 10ms/step - loss: 0.0054\n",
            "Epoch 10/50\n",
            "95/95 [==============================] - 1s 10ms/step - loss: 0.0061\n",
            "Epoch 11/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0048\n",
            "Epoch 12/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0055\n",
            "Epoch 13/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0050\n",
            "Epoch 14/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0063\n",
            "Epoch 15/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0045\n",
            "Epoch 16/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0038\n",
            "Epoch 17/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0038\n",
            "Epoch 18/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0036\n",
            "Epoch 19/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0039\n",
            "Epoch 20/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0037\n",
            "Epoch 21/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0035\n",
            "Epoch 22/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0031\n",
            "Epoch 23/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0031\n",
            "Epoch 24/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0032\n",
            "Epoch 25/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0032\n",
            "Epoch 26/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0034\n",
            "Epoch 27/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0035\n",
            "Epoch 28/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0031\n",
            "Epoch 29/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0033\n",
            "Epoch 30/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0033\n",
            "Epoch 31/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0036\n",
            "Epoch 32/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0039\n",
            "Epoch 33/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0034\n",
            "Epoch 34/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0026\n",
            "Epoch 35/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0041\n",
            "Epoch 36/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0029\n",
            "Epoch 37/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0029\n",
            "Epoch 38/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0028\n",
            "Epoch 39/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0029\n",
            "Epoch 40/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0033\n",
            "Epoch 41/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0028\n",
            "Epoch 42/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0029\n",
            "Epoch 43/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0031\n",
            "Epoch 44/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0031\n",
            "Epoch 45/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0032\n",
            "Epoch 46/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0032\n",
            "Epoch 47/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0029\n",
            "Epoch 48/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0035\n",
            "Epoch 49/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0033\n",
            "Epoch 50/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0037\n",
            "\n",
            "NVDA -\n",
            "Epoch 1/50\n",
            "95/95 [==============================] - 4s 7ms/step - loss: 0.0203\n",
            "Epoch 2/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0043\n",
            "Epoch 3/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0040\n",
            "Epoch 4/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0042\n",
            "Epoch 5/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0037\n",
            "Epoch 6/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0031\n",
            "Epoch 7/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0033\n",
            "Epoch 8/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0039\n",
            "Epoch 9/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0035\n",
            "Epoch 10/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0035\n",
            "Epoch 11/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0032\n",
            "Epoch 12/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0039\n",
            "Epoch 13/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0025\n",
            "Epoch 14/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0021\n",
            "Epoch 15/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0022\n",
            "Epoch 16/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0023\n",
            "Epoch 17/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0020\n",
            "Epoch 18/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0026\n",
            "Epoch 19/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0021\n",
            "Epoch 20/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0020\n",
            "Epoch 21/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0025\n",
            "Epoch 22/50\n",
            "95/95 [==============================] - 1s 10ms/step - loss: 0.0025\n",
            "Epoch 23/50\n",
            "95/95 [==============================] - 1s 10ms/step - loss: 0.0018\n",
            "Epoch 24/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0023\n",
            "Epoch 25/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 26/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0025\n",
            "Epoch 27/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0020\n",
            "Epoch 28/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 29/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 30/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0016\n",
            "Epoch 31/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0019\n",
            "Epoch 32/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0016\n",
            "Epoch 33/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0015\n",
            "Epoch 34/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0021\n",
            "Epoch 35/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0014\n",
            "Epoch 36/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0017\n",
            "Epoch 37/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0013\n",
            "Epoch 38/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0018\n",
            "Epoch 39/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0012\n",
            "Epoch 40/50\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 0.0022\n",
            "Epoch 41/50\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 0.0019\n",
            "Epoch 42/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0018\n",
            "Epoch 43/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0014\n",
            "Epoch 44/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0020\n",
            "Epoch 45/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0016\n",
            "Epoch 46/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0011\n",
            "Epoch 47/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0019\n",
            "Epoch 48/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0015\n",
            "Epoch 49/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0016\n",
            "Epoch 50/50\n",
            "95/95 [==============================] - 1s 7ms/step - loss: 0.0012\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_dict_small = {}\n",
        "\n",
        "for stock in x_train_dict_small:\n",
        "\n",
        "  print(stock, \"-\")\n",
        "\n",
        "  #now we are Building the LSTM network model\n",
        "  model_small = Sequential()\n",
        "  model_small.add(LSTM(units=50, return_sequences=True,input_shape=(x_train_dict_small[\"AAPL\"].shape[1],1)))\n",
        "  model_small.add(LSTM(units=50, return_sequences=False))\n",
        "  model_small.add(Dense(units=25))\n",
        "  model_small.add(Dense(units=1))\n",
        "\n",
        "  if stock == \"AAPL\":\n",
        "    print(model.summary())\n",
        "\n",
        "  # here we are Compiling the model\n",
        "  model_small.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "  # here we are training the model\n",
        "  model_small.fit(x_train_dict_small[stock], y_train_dict_small[stock], batch_size=1, epochs=50)\n",
        "\n",
        "  # Store the trained model in the model dictionary\n",
        "  model_dict_small[stock] = model_small\n",
        "\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_UH7KMrrNOJ"
      },
      "outputs": [],
      "source": [
        "# Saving all the models\n",
        "for stock in model_dict_small:\n",
        "  filename = \"keras_model_small_\" + stock + \".h5\"\n",
        "  model_dict_small[stock].save(filename)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}